{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.abspath(''), 'core'))\n",
    "from fluxonium import Fluxonium\n",
    "import general\n",
    "import utils as utl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: R2: 0.001222315716103383, EJ: 4.491477786296765, EL: 2.663656122264877, EC: 2.511293249997444\n",
      "2: R2: 0.0012214677934431824, EJ: 4.491475569333465, EL: 2.6645728736037095, EC: 2.511293249997444\n",
      "3: R2: 0.0012206300995379468, EJ: 4.49147358996162, EL: 2.665483482147963, EC: 2.511293249997444\n",
      "4: R2: 0.0012198040992560904, EJ: 4.491471846893126, EL: 2.6663867981771356, EC: 2.511293249997444\n",
      "5: R2: 0.001218992038132212, EJ: 4.491470338621796, EL: 2.667281063213268, EC: 2.511293249997444\n",
      "6: R2: 0.0012181973369324469, EJ: 4.491469063307468, EL: 2.668163536607059, EC: 2.511293249997444\n",
      "7: R2: 0.0012174253088726346, EJ: 4.491468018589055, EL: 2.669029774036696, EC: 2.511293249997444\n",
      "8: R2: 0.0012166845268705496, EJ: 4.491467201259007, EL: 2.6698721239780143, EC: 2.511293249997444\n",
      "9: R2: 0.0012159895981053131, EJ: 4.491466606643911, EL: 2.670676272349416, EC: 2.511293249997444\n",
      "10: R2: 0.0012153671691117606, EJ: 4.491466227284817, EL: 2.6714122964088194, EC: 2.511293249997444\n",
      "11: R2: 0.001214869092586941, EJ: 4.491466049645156, EL: 2.6720086787606037, EC: 2.511293249997444\n",
      "12: R2: 0.0012145903712918618, EJ: 4.491466043732168, EL: 2.672286445944126, EC: 2.511293249997444\n",
      "13: R2: 0.0012145610272655514, EJ: 4.491466124493007, EL: 2.672165015873256, EC: 2.511293249997444\n",
      "14: R2: 0.0012145602563802462, EJ: 4.491466166456293, EL: 2.6722594248991456, EC: 2.511293249997444\n",
      "15: R2: 0.0012145585123127711, EJ: 4.491466238427445, EL: 2.6721939041343066, EC: 2.511293249997444\n",
      "16: R2: 0.0012145581548205755, EJ: 4.491466289468277, EL: 2.672244412036135, EC: 2.511293249997444\n",
      "17: R2: 0.0012145577465439933, EJ: 4.491466356598016, EL: 2.6722077546855885, EC: 2.511293249997444\n",
      "18: R2: 0.0012145576104474881, EJ: 4.491466412021992, EL: 2.6722357640895513, EC: 2.511293249997444\n",
      "19: R2: 0.0012145574985683637, EJ: 4.491466476375785, EL: 2.6722150774030355, EC: 2.511293249997444\n",
      "20: R2: 0.0012145574504951065, EJ: 4.49146653412563, EL: 2.6722307826190055, EC: 2.511293249997444\n",
      "21: R2: 0.0012145574177650725, EJ: 4.491466596884535, EL: 2.6722190878237413, EC: 2.511293249997444\n",
      "22: R2: 0.0012145574013871716, EJ: 4.4914666559107514, EL: 2.6722279297315277, EC: 2.511293249997444\n",
      "23: R2: 0.001214557391546578, EJ: 4.491466717757598, EL: 2.672221318003326, EC: 2.511293249997444\n",
      "24: R2: 0.0012145573860466291, EJ: 4.491466777494396, EL: 2.6722263042598926, EC: 2.511293249997444\n",
      "25: R2: 0.001214557383063351, EJ: 4.491466838822014, EL: 2.672222567328149, EC: 2.511293249997444\n",
      "26: R2: 0.0012145573812134781, EJ: 4.49146689895712, EL: 2.6722253813795853, EC: 2.511293249997444\n",
      "27: R2: 0.0012145573803221477, EJ: 4.491466959990078, EL: 2.672223269823326, EC: 2.511293249997444\n",
      "28: R2: 0.0012145573796889205, EJ: 4.491467020349231, EL: 2.672224858547513, EC: 2.511293249997444\n",
      "29: R2: 0.0012145573794256057, EJ: 4.491467081215297, EL: 2.6722236656309657, EC: 2.511293249997444\n",
      "30: R2: 0.0012145573792028417, EJ: 4.491467141700708, EL: 2.6722245627395886, EC: 2.511293249997444\n",
      "31: R2: 0.001214557379137359, EJ: 4.491467202472363, EL: 2.672223888882013, EC: 2.511293249997444\n",
      "32: R2: 0.0012145573790468729, EJ: 4.491467263028995, EL: 2.6722243955123144, EC: 2.511293249997444\n",
      "33: R2: 0.0012145573790357483, EJ: 4.491467323747281, EL: 2.672224014888463, EC: 2.511293249997444\n",
      "34: R2: 0.0012145573789976232, EJ: 4.49146738434411, EL: 2.6722243010175037, EC: 2.511293249997444\n",
      "35: R2: 0.0012145573789958714, EJ: 4.491467445032236, EL: 2.672224086034225, EC: 2.511293249997444\n",
      "36: R2: 0.00121455737897728, EJ: 4.4914675056517614, EL: 2.6722242476407483, EC: 2.511293249997444\n",
      "37: R2: 0.001214557378978089, EJ: 4.491467566322851, EL: 2.6722241262214292, EC: 2.511293249997444\n"
     ]
    }
   ],
   "source": [
    "#Gradient Descent over T2 rate\n",
    "\n",
    "#FIXED PARAMS\n",
    "EJ = torch.rand(1, requires_grad=True, dtype=torch.double)\n",
    "EC = torch.rand(1, requires_grad=True, dtype=torch.double)\n",
    "EL = torch.rand(1, requires_grad=True, dtype=torch.double)\n",
    "flux = torch.tensor([0.5], requires_grad=True, dtype=torch.double)\n",
    "\n",
    "EJ.data = EJ.data * (20 - 2.5) + 2.5\n",
    "EC.data = EC.data * (8 - 1e-3) + 1e-3\n",
    "EL.data = EL.data * (10 - 2e-1) + 2e-1\n",
    "\n",
    "dim = 110\n",
    "\n",
    "learn_rate =1\n",
    "\n",
    "#VARIABLE PARAMS\n",
    "EJ = torch.rand(1, requires_grad=True, dtype=torch.double)\n",
    "EL = torch.rand(1, requires_grad=True, dtype=torch.double)\n",
    "EC = torch.rand(1, requires_grad=True, dtype=torch.double)\n",
    "\n",
    "EC.data = EC.data * (8 - 1e-3) + 1e-3\n",
    "EJ.data = EJ.data * (20 - 2.5) + 2.5\n",
    "EL.data = EL.data * (10 - 2e-1) + 2e-1\n",
    "\n",
    "EJ_bounds = [2.5, 150]\n",
    "EL_bounds = [2e-1, 10]\n",
    "EC_bounds = [1e-3, 8]\n",
    "\n",
    "#RECORDING VALUES\n",
    "EJ_vals = [EJ.item()]\n",
    "EL_vals = [EL.item()]\n",
    "EC_vals = [EC.item()]\n",
    "T2_rate_results = [1e100, 1e99]\n",
    "\n",
    "#we are trying to minise the rate\n",
    "\n",
    "#GRADIENT DESCENT\n",
    "i = 0\n",
    "while T2_rate_results[-1] < T2_rate_results[-2]:\n",
    "    i+=1\n",
    "\n",
    "    EJ = torch.tensor(EJ.item(), requires_grad=True, dtype=torch.double)\n",
    "    EL = torch.tensor(EL.item(), requires_grad=True, dtype=torch.double)\n",
    "    EC = torch.tensor(EC.item(), requires_grad=True, dtype=torch.double)\n",
    "\n",
    "    fluxonium = Fluxonium(EJ, EC, EL, flux, dim, \"sym_H\")\n",
    "    \n",
    "    eigvals, eigvecs = fluxonium.esys()\n",
    "\n",
    "    T2_rate  = general.t2_rate(qubit = fluxonium, eigvecs=eigvecs, eigvals =eigvals)\n",
    "\n",
    "    T2_rate.backward()\n",
    "    \n",
    "    T2_rate_results.append(T2_rate.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #If paramater value hits boundary, it stays on boundary\n",
    "        ##WHY DO WE minus the gradient and not add?\n",
    "        EJ.data = EJ - EJ.grad*learn_rate if EJ - EJ.grad*learn_rate > EJ_bounds[0] else torch.tensor(EJ_bounds[0])\n",
    "        EL.data = EL - EL.grad*learn_rate if EL - EL.grad*learn_rate > EL_bounds[0] else torch.tensor(EL_bounds[0])\n",
    "        \n",
    "    #RECORDING VALUES FOR PLOT\n",
    "    EJ_vals.append(EJ.item())\n",
    "    EL_vals.append(EL.item())\n",
    "    EC_vals.append(EL.item())\n",
    "\n",
    "    print(f\"{i}: R2: {T2_rate.item()}, EJ: {EJ.item()}, EL: {EL.item()}, EC: {EC.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Protection Factor: 76.58759942359153, EJ: 13.643962292934193, EL: 657.4980480127886, EC: 0.0010000000474974513\n",
      "2: Protection Factor: 42141172.87511427, EJ: 2.5, EL: 0.20000000298023224, EC: 9315743057.599922\n",
      "3: Protection Factor: 3.527377888277267, EJ: 2.5, EL: 0.20000000298023224, EC: 9315743057.600506\n",
      "4: Protection Factor: 3.5275465774843786, EJ: 2.5, EL: 0.20000000298023224, EC: 9315743057.60109\n",
      "5: Protection Factor: 3.5276867715022604, EJ: 2.5, EL: 0.20000000298023224, EC: 9315743057.601673\n",
      "6: Protection Factor: 3.527337370427876, EJ: 2.5, EL: 0.20000000298023224, EC: 9315743057.602257\n",
      "7: Protection Factor: 3.5274434035109654, EJ: 2.5, EL: 0.20000000298023224, EC: 9315743057.60284\n",
      "8: Protection Factor: 3.528135979404872, EJ: 2.5, EL: 0.20000000298023224, EC: 9315743057.603424\n",
      "9: Protection Factor: 3.5268782049733027, EJ: 2.5, EL: 0.20000000298023224, EC: 9315743057.604008\n",
      "10: Protection Factor: 3.527896474781875, EJ: 2.5, EL: 0.20000000298023224, EC: 9315743057.604591\n"
     ]
    }
   ],
   "source": [
    "#Gradient Descent Over Protection Factor\n",
    "\n",
    "\n",
    "#FIXED PARAMS\n",
    "EJ = torch.rand(1, requires_grad=True, dtype=torch.double)\n",
    "EC = torch.rand(1, requires_grad=True, dtype=torch.double)\n",
    "EL = torch.rand(1, requires_grad=True, dtype=torch.double)\n",
    "flux = torch.tensor([0.5], requires_grad=True, dtype=torch.double)\n",
    "\n",
    "EJ.data = EJ.data * (20 - 2.5) + 2.5\n",
    "EC.data = EC.data * (8 - 1e-3) + 1e-3\n",
    "EL.data = EL.data * (10 - 2e-1) + 2e-1\n",
    "\n",
    "dim = 110\n",
    "\n",
    "learn_rate =0.5\n",
    "\n",
    "fluxonium = Fluxonium(EJ, EC, EL, flux, dim, \"sym_H\")\n",
    "t1_noise_channels =  fluxonium.t1_supported_noise_channels()\n",
    "tphi_noise_channels =  fluxonium.tphi_supported_noise_channels()\n",
    "\n",
    "\n",
    "#VARIABLE PARAMS\n",
    "EJ = torch.rand(1, requires_grad=True, dtype=torch.double)\n",
    "EL = torch.rand(1, requires_grad=True, dtype=torch.double)\n",
    "EC = torch.rand(1, requires_grad=True, dtype=torch.double)\n",
    "\n",
    "EC.data = EC.data * (8 - 1e-3) + 1e-3\n",
    "EJ.data = EJ.data * (20 - 2.5) + 2.5\n",
    "EL.data = EL.data * (10 - 2e-1) + 2e-1\n",
    "\n",
    "EJ_bounds = [2.5, 150]\n",
    "EL_bounds = [2e-1, 10]\n",
    "EC_bounds = [1e-3, 8]\n",
    "\n",
    "delta_EJ = ((EJ_bounds[1]-EJ_bounds[0])/2)*0.01\n",
    "delta_EL = ((EL_bounds[1]-EL_bounds[0])/2)*0.01\n",
    "delta_EC = ((EC_bounds[1]-EC_bounds[0])/2)*0.01\n",
    "\n",
    "#RECORDING VALUES\n",
    "EJ_vals = [EJ.item()]\n",
    "EL_vals = [EL.item()]\n",
    "EC_vals = [EC.item()]\n",
    "protection_factor_results = [1e100, 1e99]\n",
    "\n",
    "#we are trying to minise protection factor\n",
    "\n",
    "#GRADIENT DESCENT\n",
    "i = 0\n",
    "for x in range(0,10):\n",
    "    i+=1\n",
    "\n",
    "    #Initiate qubit with current paramater values\n",
    "    EJ = torch.tensor(EJ.item(), requires_grad=True, dtype=torch.double)\n",
    "    EL = torch.tensor(EL.item(), requires_grad=True, dtype=torch.double)\n",
    "    EC = torch.tensor(EC.item(), requires_grad=True, dtype=torch.double)\n",
    "\n",
    "    fluxonium = Fluxonium(EJ, EC, EL, flux, dim, \"sym_H\")\n",
    "    \n",
    "    eigvals, eigvecs = fluxonium.esys()\n",
    "\n",
    "    T1_rate  = general.effective_t1_rate(qubit =fluxonium, eigvecs=eigvecs, eigvals =eigvals, noise_channels=t1_noise_channels)\n",
    "\n",
    "    Tphi_rate  = general.effective_tphi_rate(qubit = fluxonium, eigvecs=eigvecs,  noise_channels=tphi_noise_channels)\n",
    "\n",
    "\n",
    "    #Initiate qubit with current paramater values + delta\n",
    "    EJ_delta = torch.tensor(EJ.item()+delta_EJ, requires_grad=True, dtype=torch.double)\n",
    "    EL_delta = torch.tensor(EL.item()+delta_EL, requires_grad=True, dtype=torch.double)\n",
    "    EC_delta = torch.tensor(EC.item()+delta_EC, requires_grad=True, dtype=torch.double)\n",
    "\n",
    "    fluxonium_delta = Fluxonium(EJ_delta, EC_delta, EL_delta, flux, dim, \"sym_H\")\n",
    "    \n",
    "    eigvals, eigvecs = fluxonium.esys()\n",
    "\n",
    "    T1_rate_delta  = general.effective_t1_rate(qubit =fluxonium, eigvecs=eigvecs, eigvals =eigvals, noise_channels=t1_noise_channels)\n",
    "\n",
    "    Tphi_rate_delta  = general.effective_tphi_rate(qubit = fluxonium, eigvecs=eigvecs,  noise_channels=tphi_noise_channels)\n",
    "\n",
    "    eigvals, eigvecs = fluxonium_delta.esys()\n",
    "\n",
    "    T1_rate_delta  = general.effective_t1_rate(qubit =fluxonium_delta , eigvecs=eigvecs, eigvals =eigvals, noise_channels=t1_noise_channels)\n",
    "\n",
    "    Tphi_rate_delta  = general.effective_tphi_rate(qubit =fluxonium_delta , eigvecs=eigvecs, noise_channels=tphi_noise_channels)\n",
    "\n",
    "    #Calculate protection factor\n",
    "    protection_factor = abs(0.5*( (T1_rate - T1_rate_delta)/(Tphi_rate - Tphi_rate_delta) + (Tphi_rate - Tphi_rate_delta)/(T1_rate - T1_rate_delta))) \n",
    "\n",
    "    #Calculate gradients and apply GD\n",
    "    protection_factor.backward()\n",
    "    \n",
    "    protection_factor_results.append(protection_factor.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #If paramater value hits boundary, it stays on boundary\n",
    "        ##WHY DO WE minus the gradient and not add?\n",
    "        EJ.data = EJ - EJ.grad*learn_rate if EJ - EJ.grad*learn_rate > EJ_bounds[0] else torch.tensor(EJ_bounds[0])\n",
    "        EL.data = EL - EL.grad*learn_rate if EL - EL.grad*learn_rate > EL_bounds[0] else torch.tensor(EL_bounds[0])\n",
    "        EC.data = EC - EC.grad*learn_rate if EC - EC.grad*learn_rate > EC_bounds[0] else torch.tensor(EC_bounds[0])\n",
    "        \n",
    "        \n",
    "    #RECORDING VALUES FOR PLOT\n",
    "    EJ_vals.append(EJ.item())\n",
    "    EC_vals.append(EC.item())\n",
    "    EL_vals.append(EL.item())\n",
    "\n",
    "    print(f\"{i}: Protection Factor: {protection_factor.item()}, EJ: {EJ.item()}, EL: {EL.item()}, EC: {EC.item()}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
